{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "from util.layers import *\n",
    "from collections import OrderedDict\n",
    "from util.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourLayerNet :\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size, weight_init_std = 0.01 ):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size1)\n",
    "        self.params['b1'] = np.zeros(hidden_size1) # 은닉층1\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size1, hidden_size2)\n",
    "        self.params['b2'] = np.zeros(hidden_size2) # 은닉층2\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, hidden_size3)\n",
    "        self.params['b3'] = np.zeros(hidden_size3) # 은닉층3\n",
    "        self.params['W4'] = weight_init_std * np.random.randn(hidden_size3, output_size)\n",
    "        self.params['b4'] = np.zeros(output_size) # 출력층\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['LeakyReLU'] = LeakyReLU()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['ELU1'] = ELU()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.layers['ELU2'] = ELU()\n",
    "        self.layers['Affine4'] = Affine(self.params['W4'], self.params['b4'])\n",
    "\n",
    "        self.lastLayers = SoftmaxWithLoss()\n",
    "\n",
    "    def predict (self, x) :\n",
    "        for layer in self.layers.values() :\n",
    "            x = layer.forward(x) # 예측 단계에서는 softmax함수가 어차피 실제값과 같기 때문에 필요없음.(argmax)\n",
    "                                # 단, 학습단계에서는 loss값을 구해야 하기 때문에 softmax적용이 필요.\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t) :\n",
    "        y = self.predict(x)\n",
    "\n",
    "        return self.lastLayers.forward(y,t)\n",
    "\n",
    "    def accuracy(self, x, t) :\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / x.shape[0]\n",
    "        return accuracy\n",
    "    \n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda _: self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "\n",
    "    def gradient(self, x, t) :\n",
    "        \n",
    "        self.loss(x,t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.lastLayers.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers :\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        grads['W3'] = self.layers['Affine3'].dW\n",
    "        grads['b3'] = self.layers['Affine3'].db\n",
    "        grads['W4'] = self.layers['Affine4'].dW\n",
    "        grads['b4'] = self.layers['Affine4'].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize= True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (784, 50) (784, 50)\n",
      "W1:2.23288841097496e-13\n",
      "b1 (50,) (50,)\n",
      "b1:7.732881072671297e-13\n",
      "W2 (50, 10) (50, 10)\n",
      "W2:8.917700797641051e-13\n",
      "b2 (10,) (10,)\n",
      "b2:1.2101430968414206e-10\n"
     ]
    }
   ],
   "source": [
    "for key in grad_numerical.keys() :\n",
    "    print(key, grad_backprop[key].shape, grad_numerical[key].shape)\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + ':' + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/10000 [00:01<20:19,  8.19it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.0993, 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 613/10000 [00:09<06:18, 24.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.24883333333333332, 0.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1218/10000 [00:17<06:04, 24.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.8921333333333333, 0.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1812/10000 [00:25<05:49, 23.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9424333333333333, 0.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2411/10000 [00:33<05:10, 24.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9621, 0.9569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3020/10000 [00:41<03:42, 31.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9665833333333333, 0.9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3619/10000 [00:48<04:09, 25.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9787, 0.9689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4218/10000 [00:56<03:48, 25.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.98255, 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4809/10000 [01:04<04:34, 18.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9865333333333334, 0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5414/10000 [01:12<03:07, 24.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9833333333333333, 0.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6014/10000 [01:19<02:41, 24.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9902666666666666, 0.9744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6621/10000 [01:27<02:07, 26.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9901166666666666, 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7210/10000 [01:35<02:24, 19.33it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9952, 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7817/10000 [01:42<01:23, 26.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.99385, 0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8416/10000 [01:50<01:01, 25.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9961666666666666, 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9010/10000 [01:57<00:38, 26.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.9973666666666666, 0.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9614/10000 [02:05<00:15, 25.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc |0.99555, 0.9744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:09<00:00, 77.24it/s]\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize= True, one_hot_label = True)\n",
    "\n",
    "network = FourLayerNet(input_size=784, hidden_size1=500, hidden_size2 = 300, hidden_size3 = 100, output_size=10)\n",
    "\n",
    "train_loss_list = []\n",
    "# 추가\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "#추가\n",
    "iter_per_epoch = train_size / batch_size #정하기 나름! (ex :1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(iters_num)) :\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4') :\n",
    "        network.params[key] -= learning_rate * grad[key]  #!!!학습!!!\n",
    "\n",
    "    loss = network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    if i % iter_per_epoch == 0 :\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc |\" + str(train_acc) + \", \"+ str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973666666666666 0.9778\n"
     ]
    }
   ],
   "source": [
    "print(max(train_acc_list), max(test_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
